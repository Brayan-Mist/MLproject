"""
–£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –®–ê–ë–õ–û–ù –î–õ–Ø –ú–ê–®–ò–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø
===========================================
–ü—Ä–æ—Å—Ç–æ –∏–∑–º–µ–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ —Ä–∞–∑–¥–µ–ª–µ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–æ–≥—Ä–∞–º–º—É!
–í–µ—Ä—Å–∏—è: PyTorch
"""

import yaml
import wandb
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import os
import pickle

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

if torch.cuda.is_available():
    print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ GPU: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
    
    # –í—ã–±–æ—Ä –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—Ç–æ—Ä–∞—è - –∏–Ω–¥–µ–∫—Å 1)
    if torch.cuda.device_count() > 1:
        torch.cuda.set_device(1)
        print(f"–í—ã–±—Ä–∞–Ω–∞ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞: {torch.cuda.get_device_name(1)}")

# ============================================
# üîß –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø - –ù–ê–°–¢–†–û–ô–¢–ï –ü–û–î –°–í–û–Æ –ó–ê–î–ê–ß–£
# ============================================

def load_config(config_path="config.yaml"):
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

CONFIG = load_config()

wandb.init(
    project="universal_model_pytorch",
    config=CONFIG
)

# ============================================
# üìä –§–£–ù–ö–¶–ò–Ø –ì–ï–ù–ï–†–ê–¶–ò–ò –î–ê–ù–ù–´–• - –ò–ó–ú–ï–ù–ò–¢–ï –ü–û–î –°–í–û–Æ –ó–ê–î–ê–ß–£
# ============================================

def generate_data(n_samples):
    """
    üî• –ò–ó–ú–ï–ù–ò–¢–ï –≠–¢–£ –§–£–ù–ö–¶–ò–Æ –ü–û–î –°–í–û–ò –î–ê–ù–ù–´–ï!
    
    –ü—Ä–∏–º–µ—Ä—ã:
    1. –°—É–º–º–∞: y = x1 + x2
    2. –ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ: y = x1 * x2
    3. –ö–≤–∞–¥—Ä–∞—Ç: y = x1**2 + x2**2
    4. –°–∏–Ω—É—Å: y = np.sin(x1) + np.cos(x2)
    5. –ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ CSV: pd.read_csv('data.csv')
    """
    # –ü–†–ò–ú–ï–† 1: –°—É–º–º–∞ –¥–≤—É—Ö —á–∏—Å–µ–ª
    X = np.random.randint(0, 10, size=(n_samples, 2))
    y = X[:, 0] + X[:, 1]
    
    # –ü–†–ò–ú–ï–† 2: –ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
    # X = np.random.rand(n_samples, 2) * 10
    # y = X[:, 0] * X[:, 1]
    
    # –ü–†–ò–ú–ï–† 3: –ù–µ–ª–∏–Ω–µ–π–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å
    # X = np.random.rand(n_samples, 2) * 10
    # y = X[:, 0]**2 + np.sin(X[:, 1]) * 5
    
    # –ü–†–ò–ú–ï–† 4: –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ CSV
    # import pandas as pd
    # data = pd.read_csv('your_data.csv')
    # X = data[['feature1', 'feature2']].values
    # y = data['target'].values
    
    return X, y

# ============================================
# üß™ –§–£–ù–ö–¶–ò–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø - –ò–ó–ú–ï–ù–ò–¢–ï –ü–û–î –°–í–û–Æ –ó–ê–î–ê–ß–£
# ============================================

def calculate_expected(input_data):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –æ–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    –ò–ó–ú–ï–ù–ò–¢–ï –ø–æ–¥ –≤–∞—à—É —Ñ—É–Ω–∫—Ü–∏—é!
    """
    # –î–ª—è —Å—É–º–º—ã:
    return input_data[0] + input_data[1]
    
    # –î–ª—è –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è:
    # return input_data[0] * input_data[1]
    
    # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π:
    # return input_data[0]**2 + np.sin(input_data[1]) * 5

# ============================================
# üèóÔ∏è –û–°–ù–û–í–ù–û–ô –ö–û–î - –ù–ï –¢–†–û–ì–ê–ô–¢–ï, –ï–°–õ–ò –ù–ï –£–í–ï–†–ï–ù–´
# ============================================

# –§–∏–∫—Å–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å
torch.manual_seed(CONFIG['random_state'])
np.random.seed(CONFIG['random_state'])
if torch.cuda.is_available():
    torch.cuda.manual_seed(CONFIG['random_state'])

# –ü—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
MODEL_PATH = f"{CONFIG['model_name']}_pytorch.pth"
SCALER_PATH = f"{CONFIG['model_name']}_scaler.pkl"
PLOT_PATH = f"{CONFIG['model_name']}_training.png"

# ============================================
# üß† –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ú–û–î–ï–õ–ò
# ============================================

class NeuralNetwork(nn.Module):
    def __init__(self, input_size, layers, activation, dropout_rate, output_units, output_activation):
        super(NeuralNetwork, self).__init__()
        
        # –í—ã–±–æ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
        activation_dict = {
            'relu': nn.ReLU(),
            'tanh': nn.Tanh(),
            'sigmoid': nn.Sigmoid()
        }
        self.activation = activation_dict.get(activation, nn.ReLU())
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–µ–≤
        self.layers_list = nn.ModuleList()
        
        # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π
        self.layers_list.append(nn.Linear(input_size, layers[0]))
        self.layers_list.append(self.activation)
        if dropout_rate > 0:
            self.layers_list.append(nn.Dropout(dropout_rate))
        
        # –°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏
        for i in range(len(layers) - 1):
            self.layers_list.append(nn.Linear(layers[i], layers[i+1]))
            self.layers_list.append(self.activation)
            if dropout_rate > 0:
                self.layers_list.append(nn.Dropout(dropout_rate))
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        self.output_layer = nn.Linear(layers[-1], output_units)
        
        # –í—ã—Ö–æ–¥–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è
        if output_activation == 'softmax':
            self.output_activation = nn.Softmax(dim=1)
        elif output_activation == 'sigmoid':
            self.output_activation = nn.Sigmoid()
        else:
            self.output_activation = None
    
    def forward(self, x):
        for layer in self.layers_list:
            x = layer(x)
        x = self.output_layer(x)
        if self.output_activation:
            x = self.output_activation(x)
        return x

# ============================================
# üìà –ö–õ–ê–°–° –î–õ–Ø EARLY STOPPING
# ============================================

class EarlyStopping:
    def __init__(self, patience=15, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        self.best_model = None
        self.stop_reason = None
    
    def __call__(self, val_loss, model, current_mae=None, current_accuracy=None):
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
        if CONFIG.get('target_loss') is not None and val_loss <= CONFIG['target_loss']:
            self.early_stop = True
            self.stop_reason = f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ü–µ–ª–µ–≤–∞—è Loss: {val_loss:.6f} <= {CONFIG['target_loss']}"
            self.best_model = model.state_dict().copy()
            return
        
        if CONFIG.get('target_mae') is not None and current_mae is not None:
            if current_mae <= CONFIG['target_mae']:
                self.early_stop = True
                self.stop_reason = f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ü–µ–ª–µ–≤–∞—è MAE: {current_mae:.6f} <= {CONFIG['target_mae']}"
                self.best_model = model.state_dict().copy()
                return
        
        if CONFIG.get('target_accuracy') is not None and current_accuracy is not None:
            if current_accuracy >= CONFIG['target_accuracy']:
                self.early_stop = True
                self.stop_reason = f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ü–µ–ª–µ–≤–∞—è Accuracy: {current_accuracy:.4f} >= {CONFIG['target_accuracy']}"
                self.best_model = model.state_dict().copy()
                return
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞ Early Stopping
        if self.best_loss is None:
            self.best_loss = val_loss
            self.best_model = model.state_dict().copy()
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
                self.stop_reason = f"Early stopping: –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π {self.patience} —ç–ø–æ—Ö"
        else:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è
            improvement = self.best_loss - val_loss
            if improvement < CONFIG.get('min_improvement', 0):
                self.counter += 1
                if self.counter >= self.patience:
                    self.early_stop = True
                    self.stop_reason = f"Early stopping: —É–ª—É—á—à–µ–Ω–∏–µ < {CONFIG.get('min_improvement', 0)}"
            else:
                self.best_loss = val_loss
                self.best_model = model.state_dict().copy()
                self.counter = 0

# ============================================
# üéì –§–£–ù–ö–¶–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø
# ============================================

def train_new_model():
    """–û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
    print("\n" + "="*60)
    print("üìä –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–ê–ù–ù–´–•")
    print("="*60)
    
    X, y = generate_data(CONFIG['n_samples'])
    
    print(f"‚úì –°–æ–∑–¥–∞–Ω–æ {CONFIG['n_samples']} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"‚úì –§–æ—Ä–º–∞ X: {X.shape}, –§–æ—Ä–º–∞ y: {y.shape}")
    print(f"\n–ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
    for i in range(min(5, len(X))):
        print(f"  {X[i]} -> {y[i]}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=CONFIG['test_size'], random_state=CONFIG['random_state']
    )
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    if CONFIG['normalize']:
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
    else:
        scaler = None
        X_train_scaled = X_train
        X_test_scaled = X_test
    
    print(f"\n‚úì Train: {X_train.shape}, Test: {X_test.shape}")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä—ã PyTorch
    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)
    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)
    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)
    y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1)).to(device)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    
    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n" + "="*60)
    print("üèóÔ∏è –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò")
    print("="*60)
    
    model = NeuralNetwork(
        input_size=X.shape[1],
        layers=CONFIG['layers'],
        activation=CONFIG['activation'],
        dropout_rate=CONFIG['dropout_rate'],
        output_units=CONFIG['output_units'],
        output_activation=CONFIG['output_activation']
    ).to(device)
    
    print(model)
    print(f"\n–í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters())}")
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
    if CONFIG['loss'] == 'mse':
        criterion = nn.MSELoss()
    elif CONFIG['loss'] == 'mae':
        criterion = nn.L1Loss()
    elif CONFIG['loss'] == 'cross_entropy':
        criterion = nn.CrossEntropyLoss()
    else:
        criterion = nn.MSELoss()
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
    if CONFIG['optimizer'] == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])
    elif CONFIG['optimizer'] == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=CONFIG['learning_rate'])
    elif CONFIG['optimizer'] == 'rmsprop':
        optimizer = optim.RMSprop(model.parameters(), lr=CONFIG['learning_rate'])
    else:
        optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("\n" + "="*60)
    print("üöÄ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
    print("="*60)
    
    early_stopping = EarlyStopping(patience=CONFIG['early_stopping_patience'])
    history = {'loss': [], 'val_loss': [], 'mae': [], 'val_mae': []}
    
    for epoch in range(CONFIG['epochs']):
        # –§–∞–∑–∞ –æ–±—É—á–µ–Ω–∏—è
        model.train()
        train_loss = 0.0
        train_mae = 0.0
        
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item() * batch_X.size(0)
            train_mae += torch.mean(torch.abs(outputs - batch_y)).item() * batch_X.size(0)
        
        train_loss /= len(train_loader.dataset)
        train_mae /= len(train_loader.dataset)
        
        # –§–∞–∑–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        model.eval()
        val_loss = 0.0
        val_mae = 0.0
        
        with torch.no_grad():
            for batch_X, batch_y in test_loader:
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                
                val_loss += loss.item() * batch_X.size(0)
                val_mae += torch.mean(torch.abs(outputs - batch_y)).item() * batch_X.size(0)
        
        val_loss /= len(test_loader.dataset)
        val_mae /= len(test_loader.dataset)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
        history['loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['mae'].append(train_mae)
        history['val_mae'].append(val_mae)
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ WandB
        wandb.log({
            'epoch': epoch,
            'loss': train_loss,
            'val_loss': val_loss,
            'mae': train_mae,
            'val_mae': val_mae
        })
        
        # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        if (epoch + 1) % 10 == 0 or epoch == 0:
            print(f"Epoch {epoch+1}/{CONFIG['epochs']} - "
                  f"Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - "
                  f"MAE: {train_mae:.4f} - Val MAE: {val_mae:.4f}")
        
        # Early stopping
        early_stopping(val_loss, model, current_mae=val_mae)
        if early_stopping.early_stop:
            print(f"\n‚úì {early_stopping.stop_reason} –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
            model.load_state_dict(early_stopping.best_model)
            break
    
    print("\n‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    # –û—Ü–µ–Ω–∫–∞
    print("\n" + "="*60)
    print("üìà –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò")
    print("="*60)
    
    model.eval()
    with torch.no_grad():
        train_outputs = model(X_train_tensor)
        test_outputs = model(X_test_tensor)
        
        train_loss = criterion(train_outputs, y_train_tensor).item()
        test_loss = criterion(test_outputs, y_test_tensor).item()
        
        train_mae = torch.mean(torch.abs(train_outputs - y_train_tensor)).item()
        test_mae = torch.mean(torch.abs(test_outputs - y_test_tensor)).item()
    
    print(f"Train Loss: {train_loss:.4f}")
    print(f"Test Loss:  {test_loss:.4f}")
    print(f"Train MAE: {train_mae:.4f}")
    print(f"Test MAE:  {test_mae:.4f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plot_training(history)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': CONFIG,
        'input_size': X.shape[1]
    }, MODEL_PATH)
    
    if scaler:
        with open(SCALER_PATH, 'wb') as f:
            pickle.dump(scaler, f)
    
    print(f"‚úì –ú–æ–¥–µ–ª—å: {MODEL_PATH}")
    print(f"‚úì Scaler: {SCALER_PATH}")
    print(f"‚úì –ì—Ä–∞—Ñ–∏–∫: {PLOT_PATH}")
    
    return model, scaler

def plot_training(history):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç—É—Å–∞ GPU
    if torch.cuda.is_available():
        print(f"\nüìä –°—Ç–∞—Ç—É—Å GPU:")
        print(f"  –ü–∞–º—è—Ç—å –∑–∞–Ω—è—Ç–∞: {torch.cuda.memory_allocated()/1024**2:.0f}MB")
        print(f"  –ú–∞–∫—Å–∏–º—É–º –ø–∞–º—è—Ç–∏: {torch.cuda.max_memory_allocated()/1024**2:.0f}MB")
    
    plt.figure(figsize=(12, 4))
    
    # Loss
    plt.subplot(1, 2, 1)
    plt.plot(history['loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.legend()
    plt.grid(True)
    
    # MAE
    plt.subplot(1, 2, 2)
    plt.plot(history['mae'], label='Train MAE')
    plt.plot(history['val_mae'], label='Val MAE')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.title('Training MAE')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(PLOT_PATH)
    print(f"‚úì –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {PLOT_PATH}")

# ============================================
# üéÆ –ì–õ–ê–í–ù–û–ï –ú–ï–ù–Æ
# ============================================

print("="*60)
print(f"ü§ñ –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô ML –®–ê–ë–õ–û–ù (PyTorch)")
print(f"   –ú–æ–¥–µ–ª—å: {CONFIG['model_name']}")
print("="*60)

model_exists = os.path.exists(MODEL_PATH)

if model_exists:
    print("\n‚úì –ù–∞–π–¥–µ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å!")
    print("\n–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:")
    print("1 - –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å")
    print("2 - –û–±—É—á–∏—Ç—å –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å")
    choice = input("\n–í–∞—à –≤—ã–±–æ—Ä (1/2): ").strip()
    
    if choice == '1':
        print("\nüì• –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å...")
        checkpoint = torch.load(MODEL_PATH, map_location=device)
        
        model = NeuralNetwork(
            input_size=checkpoint['input_size'],
            layers=checkpoint['config']['layers'],
            activation=checkpoint['config']['activation'],
            dropout_rate=checkpoint['config']['dropout_rate'],
            output_units=checkpoint['config']['output_units'],
            output_activation=checkpoint['config']['output_activation']
        ).to(device)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        if CONFIG['normalize'] and os.path.exists(SCALER_PATH):
            with open(SCALER_PATH, 'rb') as f:
                scaler = pickle.load(f)
        else:
            scaler = None
        
        print("‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        print(model)
    else:
        model, scaler = train_new_model()
else:
    print("\n‚ö† –°–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    print("–ë—É–¥–µ—Ç –æ–±—É—á–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å.\n")
    model, scaler = train_new_model()

# ============================================
# üß™ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï
# ============================================

print("\n" + "="*60)
print("üéØ –†–ï–ñ–ò–ú –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
print("="*60)
print("–ö–æ–º–∞–Ω–¥—ã:")
print("  ‚Ä¢ –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
print("  ‚Ä¢ 'config' - –ø–æ–∫–∞–∑–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é")
print("  ‚Ä¢ 'test' - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç")
print("  ‚Ä¢ 'exit' - –≤—ã—Ö–æ–¥")
print("="*60 + "\n")

model.eval()

while True:
    try:
        user_input = input(">>> ").strip()
        
        if user_input.lower() == 'exit':
            print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        
        elif user_input.lower() == 'config':
            print("\n‚öôÔ∏è –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø:")
            print("="*60)
            for key, value in CONFIG.items():
                print(f"  {key:.<30} {value}")
            print("="*60 + "\n")
            continue
        
        elif user_input.lower() == 'test':
            print("\nüß™ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –¢–ï–°–¢:")
            print("="*60)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            test_X, test_y = generate_data(5)
            
            for i, numbers in enumerate(test_X):
                test_data = np.array([numbers])
                if scaler:
                    test_data = scaler.transform(test_data)
                
                test_tensor = torch.FloatTensor(test_data).to(device)
                
                with torch.no_grad():
                    prediction = model(test_tensor).cpu().numpy()[0]
                
                if CONFIG['output_units'] == 1:
                    prediction = prediction[0]
                
                expected = calculate_expected(numbers)
                error = abs(prediction - expected)
                
                status = "‚úì" if error < 1 else "‚úó"
                print(f"{status} {numbers} -> {expected:.2f} | "
                      f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: {prediction:.2f} | –û—à–∏–±–∫–∞: {error:.2f}")
            print("="*60 + "\n")
            continue
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        numbers = list(map(float, user_input.split()))
        
        if len(numbers) != CONFIG.get('input_features', 2):
            print(f"‚ùå –û—à–∏–±–∫–∞: –≤–≤–µ–¥–∏—Ç–µ {CONFIG.get('input_features', 2)} —á–∏—Å–ª–∞!\n")
            continue
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        test_data = np.array([numbers])
        if scaler:
            test_data = scaler.transform(test_data)
        
        test_tensor = torch.FloatTensor(test_data).to(device)
        
        with torch.no_grad():
            prediction = model(test_tensor).cpu().numpy()[0]
        
        if CONFIG['output_units'] == 1:
            prediction = prediction[0]
        
        expected = calculate_expected(numbers)
        error = abs(prediction - expected)
        
        print(f"\n  üìä –†–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(f"  ‚îú‚îÄ –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {numbers}")
        print(f"  ‚îú‚îÄ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:   {prediction:.2f}")
        print(f"  ‚îú‚îÄ –û–∂–∏–¥–∞–ª–æ—Å—å:      {expected:.2f}")
        print(f"  ‚îî‚îÄ –û—à–∏–±–∫–∞:         {error:.2f}")
        
        if error < 0.5:
            print("  ‚úì –û—Ç–ª–∏—á–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ! üéØ\n")
        elif error < 1.0:
            print("  ‚úì –•–æ—Ä–æ—à–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ! ‚úÖ\n")
        else:
            print("  ‚ö† –ï—Å—Ç—å –æ—à–∏–±–∫–∞... ü§î\n")
        
    except ValueError:
        print("‚ùå –û—à–∏–±–∫–∞: –≤–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —á–∏—Å–ª–∞!\n")
    except KeyboardInterrupt:
        print("\n\nüëã –ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞!")
        break
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}\n")

wandb.finish()